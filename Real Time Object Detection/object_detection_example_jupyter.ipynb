{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SAS & Open Source Packages\n",
    "Postprocessing done in JupyterLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import threading\n",
    "import time\n",
    "import websocket\n",
    "import json\n",
    "import numpy as np\n",
    "import base64\n",
    "import cv2\n",
    "import esppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "cv2.imshow('frame',gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(cv2.getBuildInformation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SAS Event Stream Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to ESP and create ESP project\n",
    "esp = esppy.ESP(hostname='http://localhost:9900')\n",
    "esp_project = esp.create_project('object_detection', n_threads=10)\n",
    "esp_project.pubsub = 'manual'\n",
    "esp_project.add_continuous_query('contquery')\n",
    "\n",
    "# Window: Video Capture\n",
    "vid_capture = esp.SourceWindow(schema=('id*:int64', 'image:blob'),\n",
    "index_type='empty', insert_only=True)\n",
    "vid_capture.pubsub = True\n",
    "esp_project.windows['w_input_image'] = vid_capture\n",
    "\n",
    "# Window: Video Resize\n",
    "vid_capture_resize = esp.CalculateWindow(algorithm='ImageProcessing', \n",
    "                                         name='resized', \n",
    "                                         function='resize',\n",
    "                                         height=416, \n",
    "                                         width=416, \n",
    "                                         input_map=dict(imageInput='image'), \n",
    "                                         output_map=dict(imageOutput='_image_'))\n",
    "vid_capture_resize.schema_string = 'id*:int64,image:blob,_image_:blob'\n",
    "esp_project.windows['w_resize_image'] = vid_capture_resize\n",
    "\n",
    "# Window: Model Reader\n",
    "model_reader = esp.ModelReaderWindow()\n",
    "esp_project.windows['w_read_model'] = model_reader\n",
    "\n",
    "# Window: Model Request\n",
    "model_request = esp.SourceWindow(schema=('req_id*:int64', 'req_key:string', 'req_val:string'),index_type='empty', insert_only=True)\n",
    "esp_project.windows['w_request_model'] = model_request\n",
    "\n",
    "# Window: Model Score\n",
    "model_score = esp.ScoreWindow()\n",
    "model_score.pubsub = True\n",
    "model_score.add_offline_model(model_type='astore')\n",
    "def score_window_fields(number_objects):\n",
    "    _field = \"id*:int64,image:blob,_image_:blob,_nObjects_:double,\"\n",
    "    for obj in range(0,number_objects):\n",
    "        _field += \"_Object\" + str(obj) + \"_:string,\"\n",
    "        _field += \"_P_Object\" + str(obj) + \"_:double,\"\n",
    "        _field += \"_Object\" + str(obj) + \"_x:double,\"\n",
    "        _field += \"_Object\" + str(obj) + \"_y:double,\"\n",
    "        _field += \"_Object\" + str(obj) + \"_width:double,\"\n",
    "        _field += \"_Object\" + str(obj) + \"_height:double,\"\n",
    "    return _field[:-1]\n",
    "model_score.schema_string = score_window_fields(20)\n",
    "esp_project.windows['w_score_image'] = model_score\n",
    "\n",
    "# Connections\n",
    "vid_capture.add_target(vid_capture_resize, role='data')\n",
    "vid_capture_resize.add_target(model_score, role='data')\n",
    "model_request.add_target(model_reader, role='request')\n",
    "model_reader.add_target(model_score, role='model')\n",
    "\n",
    "# Load Project\n",
    "esp.load_project(esp_project)\n",
    "\n",
    "# Publisher: Send Model to Scoring Window\n",
    "pub = model_request.create_publisher(blocksize=1, rate=0, pause=0, dateformat='%Y%dT%H:%M:%S.%f', opcode='insert', format='csv')\n",
    "pub.send('i,n,1,\"usegpuesp\",\"1\"\\n')\n",
    "pub.send('i,n,2,\"ndevices\",\"1\"\\n')\n",
    "pub.send('i,n,3,\"action\",\"load\"\\n')\n",
    "pub.send('i,n,4,\"type\",\"astore\"\\n')\n",
    "pub.send('i,n,5,\"reference\",\"/data/notebooks/SAS_DeepLearning/Real Time Object Detection/Tiny-Yolov2.astore\"\\n')\n",
    "pub.send('i,n,6,,\\n')\n",
    "pub.close()\n",
    "\n",
    "# Publisher: Send Video\n",
    "pub = vid_capture.create_publisher(blocksize=1, rate=0, pause=0, opcode='insert', format='csv')\n",
    "\n",
    "# Display project\n",
    "esppy.options.display.image_scale = 0.8\n",
    "esp_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Video-Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to publish videos from camera to ESP\n",
    "class video_pub():\n",
    "    def __init__(self, pub, fps):\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        self.cap.set(cv2.CAP_PROP_FOURCC, fourcc)\n",
    "        self.cap.set(3,1280)\n",
    "        self.cap.set(4,720)\n",
    "        self.pub = pub\n",
    "        self.FPS = fps\n",
    "        threading.Thread(target=self.stream, daemon=True).start()\n",
    "        print('Publisher started!')\n",
    "        \n",
    "    def stream(self):\n",
    "        prev = 0\n",
    "        while True:\n",
    "            time_elapsed = time.time() - prev\n",
    "            if time_elapsed > 1./self.FPS:\n",
    "                prev = time.time()\n",
    "                ret, frame = self.cap.read()\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                _, buffer = cv2.imencode('.jpg', frame)\n",
    "                encoded_string = base64.b64encode(buffer)\n",
    "                strToSend = 'i, n, ' + str(int(time.time()*100)) + ',' + encoded_string.decode() + ',' + '\\n'\n",
    "                self.pub.send(strToSend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Video-Subscriber to obtain scored images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to subscribe to ESP to receive and highlight scored images\n",
    "class video_sub():\n",
    "    def __init__(self):\n",
    "        self.ws = websocket.WebSocketApp(\"ws://localhost:9900/SASESP/subscribers/object_detection/contquery/w_score_image/?format=json&mode=streaming&pagesize=1\",\n",
    "                                 on_message = self.on_message,\n",
    "                                 on_error = self.on_error,\n",
    "                                 on_close = self.on_close)\n",
    "        self.ws.on_open = self.on_open\n",
    "        self.frame = None\n",
    "        threading.Thread(target=self.ws.run_forever, daemon=True).start()\n",
    "        print('Subscriber Started!')\n",
    "        return\n",
    "        \n",
    "    def highlightImage(self, data):\n",
    "        #object_list = ['Human face', 'Shirt', 'Person', 'Dress', 'Fashion accessory', 'Glasses', 'Handbag', 'Hat', 'Jewelry', 'Rifle', 'Trousers', 'Skirt', 'Weapon']\n",
    "        object_list = ['Human face', 'Shirt', 'Fashion accessory', 'Jewelry']#13\n",
    "        color_palette = [\n",
    "        (0,64,255), #red\n",
    "        (0,191,255), #orange\n",
    "        (0,255,255), #yellow\n",
    "        (0,255,64), #green\n",
    "        (255,255,0), #blue\n",
    "        (250,0,250), #pink\n",
    "        (250,0,125), #purple\n",
    "        (167,250,0), #turquoise\n",
    "        (255,200,0), #light-blue\n",
    "        (255,100,0), #dark-blue\n",
    "        (0,255,100), #light-green\n",
    "        (155,0,255), #pink\n",
    "        (255,170,0) #blue\n",
    "        ]\n",
    "        #BGR Colorcodes\n",
    "        obj_colors = {}\n",
    "        i = 0\n",
    "        for _object in object_list:\n",
    "            obj_colors[_object] = color_palette[i]\n",
    "            i += 1\n",
    "\n",
    "        row = data['events'][0]['event']\n",
    "        numberOfObjects = data['events'][0]['event']['_nObjects_']\n",
    "        imageBufferBase64 = data['events'][0]['event']['image']['image']\n",
    "\n",
    "        nparr = np.frombuffer(base64.b64decode(imageBufferBase64), dtype=np.uint8)\n",
    "        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        image_h, image_w,_ = frame.shape\n",
    "        for i in range(0, int(float(numberOfObjects))):\n",
    "            obj = row['_Object' + str(i) + '_']\n",
    "            prob = float(row['_P_Object' + str(i) + '_'])\n",
    "            probability = \" (\" + str(round(prob * 100, 2)) + \"%)\"\n",
    "            x = float(row['_Object' + str(i) + '_x'])\n",
    "            y = float(row['_Object' + str(i) + '_y'])\n",
    "            width = float(row['_Object' + str(i) + '_width'])\n",
    "            height = float(row['_Object' + str(i) + '_height'])\n",
    "            x1 = int(image_w * (x - width / 2))\n",
    "            y1 = int(image_h * (y - height/ 2))\n",
    "            x2 = int(image_w * (x + width / 2))\n",
    "            y2 = int(image_h * (y + height/ 2))\n",
    "            if obj in obj_colors:\n",
    "                bbox_color = obj_colors[obj]\n",
    "                border_offset = 3\n",
    "                cv2.rectangle(frame,(x1,y1),(x2,y2),bbox_color,1)\n",
    "                (label_width, label_height), baseline = cv2.getTextSize(obj + probability, cv2.FONT_HERSHEY_DUPLEX, 0.4, 1)\n",
    "                cv2.rectangle(frame,(x1,y1),(x1+label_width+10,y1-label_height-border_offset-10),bbox_color,-1)\n",
    "                cv2.putText(frame, obj + probability, (x1+5, y1-border_offset-5), cv2.FONT_HERSHEY_DUPLEX, 0.4, (0, 0, 0), 1,\n",
    "                    cv2.LINE_AA)\n",
    "        return frame\n",
    "\n",
    "    def on_message(self, message):\n",
    "        try:\n",
    "            data = json.loads(message)\n",
    "            self.frame = self.highlightImage(data)\n",
    "        except:\n",
    "            None\n",
    "\n",
    "    def on_error(self, error):\n",
    "        print(error)\n",
    "\n",
    "    def on_close(self):\n",
    "        print(\"### closed ###\")\n",
    "\n",
    "    def on_open(self):\n",
    "        print('open')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Video-Publisher & -Subscriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub = video_pub(pub, 25)\n",
    "sub = video_sub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display scored images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays scored frames\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    frame = sub.frame\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
